% 
\documentclass[a4paper]{article}
\usepackage[OT1]{fontenc}
\usepackage{Rd}
\usepackage[colorlinks=true]{hyperref}

\newcommand{\nmfpack}{NMF}
\newcommand{\refeqn}[1]{(\ref{#1})}


\usepackage{Sweave}
\usepackage{framed}
\usepackage[dvipsnames]{color}
\definecolor{shadecolor}{gray}{0.95}

\SweaveOpts{keep.source=TRUE}
<<options, echo=FALSE>>=
#options(prompt=' ')
options(continue=' ')
set.seed(123456)
@

\begin{document}
% \VignetteIndexEntry{Using package NMF}
% \VignetteDepends{NMF}
% \VignetteKeyword{math}

%\lstdefinestyle{Rinstyle}{style=RinstyleO, backgroundcolor=\color{gray90}}
%\lstdefinestyle{Rinstyle}{style=RinstyleO , frame=trBL , backgroundcolor=\color{gray90} , %
%numbers=left , numberstyle=\tiny , stepnumber=1,numbersep=7pt}
%\lstdefinestyle{Routstyle}{style=RoutstyleO , frame=trBL , frameround=fttt , %
%backgroundcolor=\color{gray95} , numbers=left , numberstyle=\tiny , %
%stepnumber=1,numbersep=7pt}

\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=3em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\begin{shaded}\vspace{\topsep}}{\vspace{\topsep}\end{shaded}}

\title{Using Package NMF}
\author{Renaud Gaujoux, \email{renaud@cbio.uct.ac.za}}
% \\Address Computational Biology - University of Cape Town, South Africa,

\maketitle


This vignette presents package \nmfpack, which implements a framework for 
Nonegative Matrix Factorization (NMF) algorithms in R \cite{R}. The objective is 
to provide implementation for some standard algorithms, while allowing the user 
to easily implement new methods readily integrated into the package's framework.

\medskip
The last stable version of the NMF package can be installed from any
\href{http://cran.r-project.org}{CRAN} repository mirror via:
<<install_library, eval=FALSE>>=
install.packages('NMF')
@
It loads with the following standard call:
<<load_library>>=
library(NMF)
@

\tableofcontents

\section{Overview of Nonnegative Matrix Factorization}

Let $X$ be a $n \times p$ non-negative matrix, (i.e with $x_{ij} \geq 0$,
denoted $X \geq 0$), and $r > 0$ an integer. Non-negative Matrix Factorization
(NMF) consists in finding an approximation $X \approx W H$, where $W, H$ are $n
\times r$ and $r \times p$ non-negative matrices, respectively. In practice,
the factorization rank $r$ is often chosen such that $r \ll \min(n, p)$. The
objective behind this choice is to summarize and split the information
containned in $X$ into $r$ factors: the columns of $W$. 

Depending on the application field, these factors are given different names: basis images,
metagenes, source signals. In this vignette we equivalenty and alternatively use the terms 
\emph{basis matrix} or \emph{metagenes} to refer to matrix $W$, and \emph{mixture coefficient matrix} 
and \emph{metagene expression profiles} to refer to matrix $H$.

The main approach to NMF is to estimate matrices $W$ and $H$ as a local minimum:
\begin{equation}\label{nmf_min}
\min_{W, H \geq 0}\ \underbrace{[D(X, WH) + R(W, H)]}_{=F(W,H)} \label{eq:optim_base}
\end{equation}
where 

\begin{itemize}
\item $D$ is a loss function that measures the quality of the approximation. 
Common loss functions are based on either the Frobenius distance 
$$D: A,B\mapsto Tr(AB^t) = \frac{1}{2} \sum_{ij} (a_{ij} - b_{ij})^2,$$
or the generalized Kullback-Leibler divergence.
$$D: A,B\mapsto \sum_{i,j} a_{ij} \log \frac{a_{ij}}{b_{ij}} - a_{ij} + b_{ij}.$$
\item $R$ is an optional regularization function, defined to enforce desirable
properties on matrices $W$ and $H$, such as smoothness or sparsity \cite{Cichocki04}.
\end{itemize}

\subsection{Algorithms}
Algorithms to solve problem \refeqn{nmf_min} iteratively build a sequence of matrices 
$(W_k,H_k)$ that reduces at each step the value of the objective function $F$
They differ in the optimization techniques used to compute updates for $(W_k,H_k)$.

For reviews on NMF algorithms see \cite{Berry06, Chu2004} and references therein.

Package \nmfpack\ implements a number of published algorithms, and provides a 
general framework to implement other ones.

Implemented NMF algorithms are listed or retrieved with function \code{nmfAlgorithm}.  
Specific algorithms are retrieved by their name (a \code{character} key) that is 
partially matched against the list of available algorithms:

<<nmfAlgorithm>>=
# list all available algorithms
nmfAlgorithm()
# retrieve a specific algorithm: 'brunet' 
nmfAlgorithm('brunet')
# partial match is also fine
identical(nmfAlgorithm('br'), nmfAlgorithm('brunet')) 
@   

\subsection{Initialization: seeding methods}
NMF algorithms need to be initialized with a seed (i.e. a value for $W_0$ and/or 
$H_0$\footnote{Some algorithms only need one matrix factor 
(either $W$ or $H$) to be initialized. See for example SNMF algorithms.}), 
from which to start the iteration process. 
Because there is no global minimization algorithm, and due to the problem's high 
dimensionality, the choice of the initialization is in fact very important to 
ensure meaningful results.

The more common seeding method is to start with a random guess, where the entries 
of $W$ and/or $H$ are drawn from a uniform distribution. This method is very 
simple to implement. 
However, a major drawback is that to achieve stability it requires 
to perform multiple runs, each with a different starting point. 
This significantly increases NMF algorithms' running time.  

To tackle this problem, some methods have been proposed so as to compute a 
starting point from the target matrix itself. The objective is to produce deterministic 
algorithms that need to run only once, still giving meaningful results.

For a review on some existing NMF initializations see \cite{Albright2006} and 
references therein.

Package \nmfpack\ implements a number of standard seeding methods, and provides 
a general framework to implement other ones.

Implemented seeding methods are listed or retrieved with function \code{nmfSeed}. 
Specific seeding methods are retrieved by their name (a \code{character} key) that is 
partially matched against the list of available seeding methods:  

<<nmfSeed>>=
# list all available seeding methods
nmfSeed()
# retrieve a specific method: 'nndsvd' 
nmfSeed('nndsvd')
# partial match is also fine
identical(nmfSeed('nn'), nmfSeed('nndsvd'))
@   

\subsection{How to run NMF algorithms}

Method \code{nmf} provides a single interface to run NMF algorithms. It can perform 
NMF on object of class \code{matrix}, \code{data.frame} and \code{ExpressionSet}. 
The interface takes four main parameters:

\medskip
\fbox{\code{nmf(x, rank, method='brunet', seed='random', ...)}}

\begin{itemize}
\item \code{x} is the target \code{matrix} -- \code{data.frame} or \code{ExpressionSet}
\item \code{rank} is the factorization rank
\item \code{method} is the algorithm used to estimate the factorization. 
Default algorithm is from \cite{Brunet04}. 
\item \code{seed} is the seeding method used to compute the starting point. 
Default is to use a random initialization. 
\end{itemize}

See \code{?nmf} for more details on the interface and extra parameters.

\section{Use case: Golub dataset}

The Golub dataset on leukemia used in \cite{Brunet04} is included in package 
\nmfpack. It is wrapped into an \code{ExpressionSet} object and can be loaded 
as follows. For performance reason we only use the first 1000 genes:

<<esGolub>>=
data(esGolub)
esGolub
esGolub <- esGolub[1:1000,]
@

\paragraph{Note:} To run this example, the \code{Biobase} package from BioConductor 
should be installed.

\subsection{Single run}\label{sec:single_run}

\subsubsection{Performing a single run}
The following code runs the default NMF algorithm on data \code{esGolub} 
with factorization rank equal to 3: 

<<algo_default>>=
# using default algorithm
res <- nmf(esGolub, 3) 
@

\subsubsection{Handling the result}

The result of a single NMF run is an object of class \code{NMFfit}, that holds both the fitted 
model and data about the run:

<<single_show>>=
res 
@

The fitted model can be retrieved via method \code{fit}, which returns an object of 
class \code{NMF}:

<<single_show_model>>=
fit(res)
@

The estimated target matrix can be retrieved via the generic method \code{fitted}, 
which returns a -- generally big -- \code{matrix}:

<<single_show_estimate>>=
V.hat <- fitted(res)
dim(V.hat)
@

Quality and performance measures about the factorization are computed by 
method \code{summary}:

<<singlerun_summary>>=
summary(res)
@ 

If there is some prior knowledge of classes present in the data, 
extra measures about the unsupervised clustering's performance are be computed. 
Here we use the phenotypic variable \code{Cell} that gives the samples' cell-types 
(T-cell, B-cell or \code{NA}):

<<singlerun_summary_factor>>=
summary(res, class=esGolub$Cell)
@

The basis matrix (i.e. matrix $W$ or the metagenes) and the mixture coefficient 
matrix (i.e matrix $H$ or the metagene expression profiles) are retrieved using 
methods \code{basis} and \code{coef} respectively:

<<get_matrices>>=
# get matrix W
w <- basis(res)
dim(w)

# get matrix H
h <- coef(res)
dim(h)
@


If one wants to keep only part of the result, one can directly subset on the \code{NMF}
object on features and samples (separately or simultaneously):
<<subset>>=
# keep only the first 100 features 
res[1:100,]
# keep only the first 10 samples 
res[,1:10]
# subset both features and samples:
dim(res[1:100,1:10])
@ 


\subsubsection{Selecting the features}

In general NMF matrix factors are sparse, so that the metagenes can usually be 
characterized by a relatively small set of genes. Those are determined based on 
their relative contribution to each metagene.

Kim and Park \cite{Kim2007} defined a procedure to extract the relevant genes for each 
metagene, based on a gene scoring schema.

The NMF package implements this procedure in methods \code{featureScore} and 
\code{extractFeature}:

<<single_extract>>=
# only compute the scores
s <- featureScore(res)
summary(s)

# compute the scores and characterize each metagene
s <- extractFeatures(res)
str(s)
@

\subsection{Specifying the algorithm}  
The algorithm used to compute the NMF is specified in the third argument (\code{method}). 
For example, to use the Nonsmooth NMF algorithm from \cite{nsNMF2006}: 
<<algo_ns>>=
# using the Nonsmooth NMF algorithm with parameter theta=0.7
res <- nmf(esGolub, 3, 'ns', theta=0.7)
res
@

\subsection{Multiple runs}

The default seeding method being random seeding, multiple runs are required to 
achieve stability. This can be done by setting argument \code{nrun} to the desired 
value. For performance reason we use \code{nrun=5} here, but a reasonnable choice 
would typically lies between 100 and 200:  

<<algo_multirun>>=
res.multirun <- nmf(esGolub, 3, nrun=5)
res.multirun
@

As we can see from the results above, the returned object contains only one fit, 
from the 5 runs that was perfomed.
The default behaviour is to only keep the factorization achieving the lowest 
approximation error (i.e. the lowest objective value).
However if one is interested in keeping the 
results from all the runs, one can set the option \code{keep.all=TRUE}:

<<multirun_keep, eval=FALSE>>=
# using letter code 'k' in argument .options
nmf(esGolub, 3, nrun=5, .options='k')
# or explicitly setting the option
nmf(esGolub, 3, nrun=5, .options=list(keep.all=TRUE))
@   

\subsection{Specifying the seeding method}
The seeding method used to compute the starting point for the chosen 
algorithm can be set via argument \code{seed}. Note that if the seeding method is 
deterministic there is no need to perform multiple run anymore:

<<seed>>=
res <- nmf(esGolub, 3, seed='nndsvd')
res
@

Another possibility, useful when comparing methods, is to set the seed of the random generator passing a 
numerical value in argument \code{seed}. In this case, function \code{set.seed} 
from package \code{base} is called before using seeding method \code{'random'}:

<<seed_numeric>>=
res <- nmf(esGolub, 3, seed=123456)
res
@

\subsection{Visualization methods}

\subsubsection*{Error track}

If the NMF computation is performed with error tracking enabled -- using argument 
\code{.options} -- the trajectory of the objective value can be plot with 
method \code{errorPlot} (see Figure \ref{fig:error}): 

<<errorplot_compute, include=FALSE>>=
res <- nmf(esGolub, 3, .options='t')
# or alternatively:
# res <- nmf(esGolub, 3, .options=list(track=TRUE))
errorPlot(res)
@

\begin{figure}[ht]
\centering
<<errorplot_include, fig=true, echo=FALSE>>=
<<errorplot_compute>>
@
\caption{Error track for a single NMF run}
\label{fig:error}
\end{figure}

\subsubsection*{Heatmaps}

Method \code{metaHeatmap} provides an easy way to vizualize the resulting 
metagenes, metaprofiles and, in the case of multiple runs, the consensus matrix. 
It produces pre-configured heatmaps based on function \code{heatmap.2} from 
package \code{gplots}. Examples of those heatmaps are shown in figures~\ref{fig:heatmap_profiles}, 
\ref{fig:heatmap_genes}, \ref{fig:heatmap_consensus} and
\ref{fig:heatmap_consensus_precomp}.

The following -- default -- call plots the metaprofiles matrix (see result 
Figure~\ref{fig:heatmap_profiles}):
<<heatmap_profile, include=FALSE>>=
# default is to plot metaprofiles
metaHeatmap(res) 
@

\begin{figure}[ht]
\centering
<<heatmap_profile_inc, fig=true, echo=FALSE>>=
<<heatmap_profile>>
@
\caption{Heatmap of metaprofiles}
\label{fig:heatmap_profiles}
\end{figure}

The metagenes matrix can be plotted specifying the second argument
\code{what} (see result Figure~\ref{fig:heatmap_genes}). 
We use argument \code{filter} to select only the genes that are specific to 
each metagene. 
With \code{filter=TRUE}, the selection method is the one described in \cite{Kim2007}.
<<heatmap_genes, include=FALSE>>=
metaHeatmap(res, what='features', filter=TRUE)
@

\begin{figure}[ht]
\centering
<<heatmap_genes_inc, fig=true, echo=FALSE>>=
<<heatmap_genes>>
@
\caption{Heatmap of metagenes}
\label{fig:heatmap_genes}
\end{figure}


In the case of multiple runs method \code{metaHeatmap} plots the consensus
matrix, i.e. the average connecticity matrix accross the runs (see results
Figures~\ref{fig:heatmap_consensus} and \ref{fig:heatmap_consensus_precomp}
for a consensus matrix obtained with 100 runs of Brunet's algorithm on Golub
dataset):
<<heatmap_consensus, include=FALSE>>=
# The cell type is used to label rows and columns 
metaHeatmap(res.multirun, labRow=esGolub$Cell, labCol=esGolub$Cell)
@


\begin{figure}[ht]
\centering
<<heatmap_consensus_inc, fig=true, echo=FALSE>>=
<<heatmap_consensus>>
@
\caption{Heatmap of consensus matrix}
\label{fig:heatmap_consensus}
\end{figure}
    
\begin{figure}[ht]
\centering
\includegraphics{consensus}
\caption{Heatmap of consensus matrix (100 runs of Brunet's algorithm on Golub
dataset)}
\label{fig:heatmap_consensus_precomp}
\end{figure}

\subsection{Comparing algorithms}
To compare the results from different algorithms, one can pass a list of methods 
in argument \code{method}. To enable a fair comparison, a deterministic seeding 
method should also be used. Here we fix the random seed to 123456. 

<<multimethod>>=
res.multi.method <- nmf(esGolub, 3, list('brunet', 'lee', 'ns'), seed=123456)
@

Passing the result to method \code{compare} produces a \code{data.frame} 
that contains summary measures for each method. Again, prior knowledge of classes 
may be used to compute clustering quality measures:  

<<compare>>=
compare(res.multi.method)

# If prior knowledge of classes is available
compare(res.multi.method, class=esGolub$Cell)
@

When the computation is performed with error tracking enabled, an error plot is 
produced by method \code{errorplot} (see figure~\ref{fig:multi_error}):

<<multiple_errorplot_compute, include=FALSE>>=
res <- nmf(esGolub, 3, list('brunet', 'lee', 'ns'), seed=123456, .options='t')
errorPlot(res)
@

\begin{figure}[ht]
\centering
<<multiple_errorplot_include, fig=true, echo=FALSE>>=
<<multiple_errorplot_compute>>
@
\caption{Error tracks comparing methods \texttt{'brunet', 'lee', 'nsNMF'}}
\label{fig:multi_error}
\end{figure}


 
\section{Advanced usage}

We developped package \nmfpack\ with the objective to allow the integration 
of new NMF methods, trying to impose only few requirements on their 
implementations. 
All the built-in algorithms and seeding methods are implemented as strategies 
that are called from within the main interface method \code{nmf}. 

The user can define new strategies and those are handled in exactly the same way as 
the built-in ones, benefiting from the same utility functions to interpret the 
results and assess their performance. 

\subsection{Custom algorithm}
%New NMF algrithms can be defined in two ways:
%
%\begin{itemize}
%\item as a single \code{function} 
%\item as a set of functions that implement a pre-defined \emph{iterative schema}
%\end{itemize}
%
%\subsubsection{Defined as a \code{function}}
To define a strategy, the user needs to provide a \code{function} that 
implements the complete algotihm. It must be of the form: 

<<custom_algo_sig>>=
my.algorithm <- function(target, start, param.1, param.2){
	# do something with starting point
	# ...
	
	# return updated starting point
	return(start)
}
@
Where:

\begin{description}
\item[target] is a \code{matrix}; 
\item[start] is an object that inherits from class \code{NMF}. This \code{S4} 
class is used to handle NMF models (matrices \code{W} and \code{H}, objective 
function, etc\dots);
\item[param.1, param.2] are extra parameters specific to the algorithms;
\end{description}

The function must return an object that inherits from class \code{NMF}

For example:
<<custom_algo>>=
my.algorithm <- function(target, start, scale.factor=1){
	# do something with starting point
	# ...
	# for example: 
	# 1. compute principal components	
	pca <- prcomp(t(target), retx=TRUE)
	# 2. use the absolute values of the first PC for the metagenes
	# Note: the factorization rank is stored in object 'start'
	factorization.rank <- nbasis(start)
	metagenes(fit(start)) <- abs(pca$rotation[,1:factorization.rank])
	# use the rotated matrix to get the mixture coefficient
	# use a scaling factor (just to illustrate the use of extra parameters)
	metaprofiles(fit(start)) <- t(abs(pca$x[,1:factorization.rank])) / scale.factor
	
	# return updated data
	return(start)
}
@

To use the new method within the package framework, one pass \code{my.algorithm} 
to main interface \code{nmf} via argument \code{method}. Here we apply the 
algorithm to some matrix \code{V} randomly generated: 

<<define_V>>=
n <- 50; r <- 3; p <- 20
V <-syntheticNMF(n, r, p, noise=TRUE)
@

<<custom_algo_run>>=
nmf(V, 3, my.algorithm, scale.factor=10)
@

The default distance measure is based on the euclidean distance. 
If the algorithm is based on another distance measure, this one can be 
specified in argument{objective}, either as a \code{character} string 
corresponding to a built-in objective function, or a custom \code{function} 
definition:

<<custom_algo_run_obj>>=
# based on Kullbach-Leibler divergence
nmf(V, 3, my.algorithm, scale.factor=10, objective='KL')
# based on custom distance metric
nmf(V, 3, my.algorithm, scale.factor=10
	, objective=function(target, x){ 
			( sum( (target-fitted(x))^4 ) )^{1/4} 
		}
)
@

%\subsubsection{Using the iterative schema} 
%
%NMF algorithms generally implement the following common iterative schema:
%
%\begin{enumerate}
%\item
%\item 
%\end{enumerate}

\subsection{Handling mixed sign data}
Some NMF algorithms work with relaxed constraints, where the input data and one of the matrix factors are allowed to have negative entries (eg. semi-NMF).
One can plug such algorithms into the framework defined by package NMF, by specifying argument \code{mixed=TRUE}.
The default value for argument \code{mixed} is \code{FALSE}, so that method \code{nmf} throws an error if applied to a matrix with some negative entries.
Note that the sign of the factors is not checked, so that one can always return factors with negative entries, independently of the value of argument \code{mixed}.
Here we reuse the previously defined custom algorithm\footnote{As it is defined here, the custom algorithm still returns nonnegative factors, which would not be desirable in a real example.}:


<<custom_algo_run_mixed>>=
# put some negative input data 
V.neg <- V; V.neg[1,] <- -1;

# this generates an error
err <- try( nmf(V.neg, 3, my.algorithm, scale.factor=10) )
err

# this runs my.algorithm without error
nmf(V.neg, 3, my.algorithm, mixed=TRUE, scale.factor=10)
@


\subsection{Custom seeding method}

The user can also define custom seeding method as a function of the form:


<<custom_seed>>=

# start: object of class NMF
# target: the target matrix
my.seeding.method <- function(model, target){
	
	# use only the largest columns for W
	w.cols <- apply(target, 2, function(x) sqrt(sum(x^2)))
	metagenes(model) <- target[,order(w.cols)[1:nbasis(model)]]
	
	# initialize H randomly
	metaprofiles(model) <- matrix(runif(nbasis(model)*ncol(target))
						, nbasis(model), ncol(target))

	# return updated object
	return(model)
}
@

To use the new seeding method:
<<custom_algo_run>>=
nmf(V, 3, 'snmf/r', seed=my.seeding.method)
@

\addcontentsline{toc}{section}{References}
\begin{thebibliography}{}

\bibitem[Albright et al., 2006]{Albright2006} R. Albright, J. Cox, D. Duling, A. Langville, C. Meyer (2006).
Algorithms, initializations, and convergence for the nonnegative matrix factorization.
{\it NCSU Technical Report Math 81706}. \url{http://meyer.math.ncsu.edu/Meyer/Abstracts/Publications.html}.

\bibitem[Berry {\it et al}., 2006]{Berry06} Berry et al. (2006). Algorithms and
Applications for Approximate Nonnegative Matrix Factorization. {\it Comput.
Stat. Data Anal.}

\bibitem[Brunet {\em et~al.}, 2004]{Brunet04} Brunet, J.~P., Tamayo, P., Golub, T.~R., and Mesirov, J.~P. (2004). Metagenes and molecular pattern discovery using matrix factorization. {\em Proc Natl Acad Sci U S A\/}, {\bf 101}(12), 4164--4169.

\bibitem[A. Cichocki {\it et al}., 2004]{Cichocki04} Andrzej Cichocki , Rafal Zdunek, and Shun-ichi Amari (2004). New algorithms For Non-negative Matrix Factorization In Application To Blind Source Separation.

\bibitem[Chu {\it et al.}, 2004]{Chu2004} M.T. Chu, F. Diele, R. Plemmons, S. Ragni. Optimality, computation, and interpretation of nonnegative matrix factorizations. {\it Technical Report}, Departments of Mathematics and Computer Science, Wake Forest University, USA.

\bibitem[Kim and Park, 2007]{Kim2007}
Kim H, Park H: \textbf{{Sparse non-negative matrix factorizations via
  alternating non-negativity-constrained least squares for microarray data
  analysis.}} \emph{Bioinformatics (Oxford, England)} 2007,
  \textbf{23}:1495--502,
  \url{[http://www.ncbi.nlm.nih.gov/pubmed/17483501]}.

\bibitem[Lee and Seung, 2000]{LeeSeung00} Lee, D.~D. and Seung, H.~S. (2000). Algorithms for non-negative matrix factorization. In {\it {NIPS}\/}, 556-562.

\bibitem[Pascual-Montano {\em et~al.}, 2006]{nsNMF2006} Pascual-Montano, A., Carazo, J.~M., Kochi, K., Lehmann, D., and Pascual-Marqui, R.~D. (2006). Nonsmooth nonnegative matrix factorization (nsnmf). {\em IEEE transactions on pattern analysis and machine intelligence\/}, {\bf 28}(3), 403-415.

\bibitem[R Software, 2008]{R} R Development Core Team.
R: A Language and Environment for Statistical Computing.
Vienna, Austria. ISBN 3-900051-07-0. \url{http://www.R-project.org}.
\end{thebibliography}

\end{document}
